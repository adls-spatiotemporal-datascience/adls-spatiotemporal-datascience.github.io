---
title: "DuckDB"
---

[]{.lts .content-hidden unless-profile="book"}


- DuckDB is a relatively new, open-source, column-oriented relational database management system (RDBMS)
- It is designed to provide high performance on complex queries against large databases in embedded configuration
- It has a flexible [extension](https://duckdb.org/docs/stable/extensions/overview) mechanism consisting of *core* and *community* extensions
- [spatial](https://duckdb.org/docs/stable/extensions/spatial/overview.html) is a core extension
- DuckDB is a *OLAP*, *in process* RDBMS (see below)

- For a great introduction by DuckDB's co-creator, watch [Hannes Mühleisen — DuckDB, an in-process analytical DBMS](https://www.youtube.com/watch?v=Z-6SnP6yzgo) (NY Open Statistical Programming Meetup, 2022)

---

### Why DuckDB for spatial data?

- Tools like R/`sf` load entire datasets into memory — this becomes a bottleneck with large spatial datasets
- DuckDB can process data that exceeds available RAM by streaming from disk
- SQL is a widely used, declarative language — you describe *what* you want, not *how* to compute it
- The spatial extension provides familiar functions (`st_intersects`, `st_intersection`, `st_area`, ...) — the same names as in `sf`
- Combined with spatial indexing (R-Tree), DuckDB can execute spatial queries on large datasets much faster than in-memory approaches

---

### Data science tools are reinventing DBMS

- Tools like `dplyr`, `data.table`, and `arrow` perform relational data transformations (select, join, aggregate, ...) — this is core DBMS functionality
- But compared to a real DBMS, they lack:
  - **Holistic query optimization** across a multi-step pipeline
  - **Out-of-memory computation** — when intermediates exceed RAM, things break
  - **Parallelism and pipelining** — limited use of multiple CPU cores
- DuckDB brings these DBMS capabilities directly into your R/Python session
- DuckDB is designed for analytical usecases (OLAP)

:::{.notes}
> The data science community is reinventing database management systems, but rather poorly." If you build a dplyr or data.table pipeline, there is no optimization step that looks at the entire pipeline — it just materializes after each step, leading to potentially huge intermediate results. Combined with no out-of-memory support, things simply stop working. DuckDB solves this by bringing 50+ years of DBMS research into an embeddable package.

Hannes Mühleisen (DuckDB co-creator) makes this point in his [talk](https://www.youtube.com/watch?v=Z-6SnP6yzgo): "

:::

## OLAP vs. OLTP

- OLAP (Online Analytical Processing)
  - Read-mostly workloads
  - Complex queries
  - read large parts of the data
  - bulk appends / updates
- OLTP (Online Transaction Processing)
  - Many small writes and updates
  - simple queries
  - read only individual rows


## In process vs. standalone

- DuckDB is an *in process* database management system, it is not an external process to which your application connects. 
- In other words: there is no client sending instructions nor a server to read and process them
- SQLite works the same way, while PostgreSQL, MySQL etc. do not


```{r}
#| echo: false
#| label: tbl-duckdb
#| tbl-cap: DuckDB fills a niche that no previous software has filled yet

library(gt)

df2 <- c("sqlite","duckdb","postgres","clickhouse") |> 
  (\(x)file.path("images","logos",paste0(x, ".png")))() |> 
  matrix(ncol = 2, byrow = TRUE, dimnames = list(c("In-Process","Stand-Alone"),c("OLTP","OLAP"))) |> 
  data.frame()
  
df2 |> 
  gt(rownames_to_stub = TRUE) |> 
  fmt_image(columns = c(OLTP, OLAP), width = "10em")

```


## Duckdb in practice {#sec-duckdb-practice}

- Download `wald-kantone.duckdb` from moodle (forest + canton boundaries)
- Install the [duckdb CLI](duckdb.org/docs/installation/?version=stable&environment=cli) and [duckdb R package](https://duckdb.org/docs/installation/?version=stable&environment=r)
- Install the [dbeaver Community Version](https://dbeaver.io/download/)
- Using *dbeaver*, connect to the duckdb database
- Install and load the spatial extension:
   
```{.sql}
INSTALL spatial;
LOAD spatial;
```

---

- Verify both tables are present and explore them:

```{.sql}
SHOW TABLES;
SELECT * FROM wald;
SELECT * FROM kantone;
```

- Create an R-Tree spatial index for both tables:

```{.sql}
CREATE INDEX kantone_idx ON kantone USING RTREE (geom);
CREATE INDEX wald_idx ON wald USING RTREE (geom);
```

---

### Why spatial indices?

- Without an index, a spatial query like `WHERE st_intersects(a, b)` must check **every** pair of geometries — this is very slow on large datasets
- An **R-Tree** index organizes geometries by their *bounding boxes* in a tree structure
- When querying, the R-Tree quickly eliminates geometries whose bounding boxes don't overlap — only the remaining candidates are checked with the exact (expensive) geometry test
- This reduces the number of comparisons dramatically (e.g. from millions to thousands)
- R-Trees are the standard spatial index in most databases (PostGIS, SpatiaLite, DuckDB) and also power `sf`'s spatial predicates internally

---

### SQL `VIEW`

- A `VIEW` is a **named SQL query** — it acts like a virtual table
- Every time you access a `VIEW`, the underlying query is re-executed
- This lets us build complex analyses step-by-step (rather than nesting queries)

:::{.notes}
Unlike materialized tables, VIEWs don't store data on disk. This means they don't take up extra space and always reflect the current state of the source data. The trade-off is that they are re-computed on every access. We will use VIEWs to recreate the forest-per-canton analysis from Task 1 in SQL.
:::

- Our first `VIEW`: a subset of the forest dataset (for faster iteration)

---

- Limit to 1'000 rows and store as a `VIEW`:

```{.sql}
CREATE VIEW wald2 AS            -- <1>
SELECT * FROM wald LIMIT 1000;  -- <2>
```

1. Prepend `CREATE VIEW name AS` to any `SELECT`...
2. ... to store it as a reusable virtual table

- Query it like a regular table:

```{.sql}
SELECT * FROM wald2;
```

---

### Develop SQL Code

- Intersect forest polygons with canton boundaries:

```{.sql}
SELECT 
  name, 
  st_intersection(w.geom, k.geom),  -- <2>
FROM wald2 w, kantone k;            -- <1>
```

1. `w` and `k` are aliases...
2. ... used in the intersection


---

- Problem: this computes a **cross join** — *every* forest polygon x *every* canton (1'000 x 26 = 26'000 pairs)
- Solution: add a `WHERE` clause to filter early:

```{.sql}
SELECT
  name,
  st_intersection(w.geom, k.geom),
FROM wald2 w, kantone k
WHERE st_intersects(w.geom, k.geom); -- <1>
```

1. Only compute the intersection for pairs that actually overlap

- This is where the **R-Tree index** pays off — DuckDB discards non-overlapping pairs without checking every combination

:::{.callout-note}
## `st_intersects` vs `st_intersection`
Note how this query uses both: `st_intersects` in the `WHERE` clause is a **predicate** (returns true/false to filter rows), while `st_intersection` in the `SELECT` clause is a geometric **operation** (computes the overlapping geometry). Same pattern as in R/`sf`!
:::

---

### Inspecting query plans with `EXPLAIN`

- You can prepend `EXPLAIN` to any query to see *how* DuckDB will execute it, without actually running it
- Try comparing the two versions in dbeaver:

```{.sql}
-- Without WHERE: cross join of all pairs
EXPLAIN SELECT name, st_intersection(w.geom, k.geom)
FROM wald2 w, kantone k;

-- With WHERE: only matching pairs (uses R-Tree index)
EXPLAIN SELECT name, st_intersection(w.geom, k.geom)
FROM wald2 w, kantone k
WHERE st_intersects(w.geom, k.geom);
```

- In the second plan, look for an `INDEX SCAN` or `INDEX JOIN` — this confirms the R-Tree is being used

---

- We need the **area** of each intersection, not the geometry itself:


```{.sql}
SELECT 
  name, 
  st_area(st_intersection(w.geom, k.geom)) as wald_area, -- <1>
FROM wald2 w, kantone k
WHERE st_intersects(w.geom, k.geom);
```

1. `st_area` calculates the area of the intersection

---

- Save this as a `VIEW` before aggregating:

```{.sql}
CREATE VIEW wald_kantone AS            -- <1>
SELECT 
  name, 
  st_area(st_intersection(w.geom, k.geom)) AS wald_area,
FROM wald2 w, kantone k
WHERE st_intersects(w.geom, k.geom);
```

1. This creates a `VIEW` from the preceding query

---

- Query the `VIEW` like a table, then aggregate with `GROUP BY`:

```{.sql}
SELECT 
  name,                         -- <3>
  sum(wald_area) as wald_area   -- <2>
FROM wald_kantone
GROUP BY name;                  -- <1>
``` 

1. If we use `GROUP BY` in a SQL query..
2. ... we need to wrap all columns with aggregate function...
3. ... except for the columns that we use for grouping

---

- Save the aggregation as a `VIEW`:

```{.sql}
CREATE VIEW wald_kanton_grp AS
SELECT 
  name, 
  sum(wald_area) as wald_area
FROM wald_kantone
GROUP BY name;
```

---

- Join with `kantone` to get the total canton area and compute the fraction:

```{.sql}
SELECT 
	kantone.name,                          -- <3>
	wald_area/area as waldanteil,          -- <4>
FROM wald_kanton_grp 
LEFT JOIN kantone                        -- <1>
ON wald_kanton_grp.name=kantone.name;    -- <2>
```

1. `LEFT JOIN` appends columns from another table...
2. ... matched by the `ON` condition
3. We only need the canton name...
4. ... and the fraction `wald_area / area`

---

- Save as a final `VIEW`, ordered by forest share:

```{.sql}
CREATE VIEW kanton_frac AS
SELECT 
	kantone.name,                 
	wald_area/area as waldanteil, 
FROM wald_kanton_grp 
LEFT JOIN kantone 
ON wald_kanton_grp.name=kantone.name
ORDER BY waldanteil DESC;           -- <1>
```

1. We can `ORDER BY` to show us the highest values first

---

### Scaling up: from subset to full dataset

- So far we only used 1'000 forest features — results are incomplete
- Since we used `VIEW`s, switching to the full dataset is trivial:

```{.sql}
CREATE OR REPLACE VIEW wald2 AS
SELECT * FROM wald;
```

- Now every downstream `VIEW` automatically uses the full data:

```{.sql}
SELECT * FROM kanton_frac;
```

:::{.notes}
`CREATE OR REPLACE VIEW` is needed because `wald2` already exists. This is the key advantage of VIEWs: by replacing just one definition, the entire chain of dependent queries updates automatically. The downside is that the full query now takes longer since nothing is materialized.
:::

---

### `VIEW` vs `CREATE TABLE ... AS`

- A `VIEW` is lazy — the query is re-executed every time you access it
- If performance matters, you can **materialize** intermediate results using `CREATE TABLE ... AS`:

```{.sql}
CREATE TABLE wald_kantone_mat AS           -- <1>
SELECT
  name,
  st_area(st_intersection(w.geom, k.geom)) AS wald_area
FROM wald2 w, kantone k
WHERE st_intersects(w.geom, k.geom);
```

1. This stores the result as an actual table on disk

- Trade-off: materialized tables are faster to query, but they take up disk space and do **not** update automatically when the source data changes
- VIEWs are the opposite: no extra storage, always up-to-date, but slower to query

## Import results into R

- Connect from R, load the spatial extension, and read the `VIEW`:

```{r}
#| eval: false
library(duckdb)

con <- dbConnect(
  duckdb(),
  dbdir = "data/week4-exercises/wald-kantone.duckdb",
  read_only = TRUE
)

dbExecute(con, "LOAD spatial;")
kanton_frac <- dbReadTable(con, "kanton_frac")

dbDisconnect(con)
```

